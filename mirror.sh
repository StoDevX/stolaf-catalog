#!/bin/bash
set -ve

# -e robots=off: ignore robots.txt
# --mirror: equivalent to -r -N -l inf --no-remove-listing
	# -r: Turn on recursive retrieving. The default maximum depth is 5.
	# -l: Specify recursion maximum depth level depth.
	# -N: Turn on time-stamping.
	# --no-remove-listing Don't remove the temporary .listing files generated by FTP retrievals.
# --page-requisites: This option causes Wget to download all the files that are necessary to properly display a given HTML page.  This includes such things as inlined images, sounds, and referenced stylesheets.
# --no-if-modified-since: Do not send If-Modified-Since header in -N mode. Send preliminary HEAD request instead.
# --convert-links: After the download is complete, convert the links in the document to make them suitable for local viewing.
# --domains: Set domains to be followed. `domain-list` is a comma-separated list of domains.
# --exclude-directories=list
   # Specify a comma-separated list of directories you wish to exclude
   # from download.  Elements of list may contain wildcards.

function join_by { local IFS="$1"; shift; echo "$*"; }

ignore=(
	/admin/
	/catalogcontents/
	/cim/
	/clmail/
	/courseadmin/
	/courseleaf/
	/dbleaf/
	/import/
	# /images/
	/fonts/
	/js/
	/mig/
	/migration/
	/navbar/
	/pagewiz/
	/programadmin/
	/responseform/
	/ribbit/
	/search/
	/sectionrequest/
	/shared/
	/tmp/
	/wen/
	/wiztest/
	/xsearch/
)

ignore_list=$(join_by , ${ignore[*]})

wget \
	-e robots=off \
	--mirror \
	--page-requisites \
	--no-if-modified-since \
	--convert-links \
	--domains catalog.stolaf.edu \
	--exclude-directories=$ignore_list \
	http://catalog.stolaf.edu
